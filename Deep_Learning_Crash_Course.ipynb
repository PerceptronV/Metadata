{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Crash Course.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOo/KKWeU0xnaRZ1lu/nRvN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PerceptronV/Miscellaneous/blob/master/Deep_Learning_Crash_Course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyFs-njZfYpZ",
        "colab_type": "text"
      },
      "source": [
        "#DLCC-10\n",
        "####A 10 minute crash course in Deep Learning\n",
        "\n",
        "/_By Vincent Song_\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R76bt0FavSGW",
        "colab_type": "text"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/PerceptronV/Miscellaneous/blob/master/Deep_Learning_Crash_Course.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/PerceptronV/Miscellaneous/blob/master/Deep_Learning_Crash_Course.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzwKE_eU9WZZ",
        "colab_type": "text"
      },
      "source": [
        "##Introduction\n",
        "```\n",
        "Artificial intelligence has taken the world by storm. But how exacty does it work, and what lies ahead?\n",
        "```\n",
        "\n",
        "We'll be exploring all these and some more. By the end of this quick, interactive notebook, you'll have have trained your own text-generating AI!\n",
        "\n",
        "Let's take a glimpse of greatness:\n",
        "![A blend of human and machine-generated Haikus](https://github.com/PerceptronV/Miscellaneous/raw/master/haiku_guess.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qza4MfAvCF1",
        "colab_type": "text"
      },
      "source": [
        "##A Map of AI\n",
        "![3 Stages of AI](https://image.slidesharecdn.com/futureofai20190507v7-190507093643/95/future-of-ai-20190507-v7-5-638.jpg?cb=1557221878)\n",
        "\n",
        "_After Dario Gil, IBM_\n",
        "\n",
        "| Narrow AI | Broad AI | General AI |\n",
        "| ------------- | ------------- | ------------- |\n",
        "| Single-task, single-domain | Multi-task, multi-domain | Cross-domain learning and reasoning |\n",
        "| Superhuman performace on specific tasks | Explainable | Broad autonomy, superhuman performace in every area |\n",
        "\n",
        "\\\n",
        "\n",
        "\\\n",
        "\n",
        "![The map of AI](https://top6sites.com/wp-content/uploads/2020/05/AI-vs-ML-vs-Deep-Learning.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGixjPGBCrqa",
        "colab_type": "text"
      },
      "source": [
        "##A bit on theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjMoVOZZXVDf",
        "colab_type": "text"
      },
      "source": [
        "###Let's get our hands dirty!\n",
        "_Time for the code to come in!_\n",
        "\n",
        "For this simple demonstration, we will be coding in Python - the leading language for data scientists.\n",
        "\n",
        "__SPOILER ALERT__: Implementing a basic neural network and training it from scratch is going to take an absurdly long time, and is well beyond what we can do in 10 minutes. Therefore, to speed up the process, we will be using _TensorFlow_, a popular machine learning library, and _Keras_, a framework built on top of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFy7x46c-vre",
        "colab_type": "text"
      },
      "source": [
        "###Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flIeL0wXfNIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let us first import the libraries we need\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import time, os, math\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdOzm6dMf_nc",
        "colab_type": "text"
      },
      "source": [
        "###Creating a simple GRU layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DNpJiuNgCEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvICge0n9bSY",
        "colab_type": "text"
      },
      "source": [
        "##Creating our AI system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N05NDOe__FKx",
        "colab_type": "text"
      },
      "source": [
        "###Downloading the data\n",
        "We are going to teach an AI how to generate realistic-looking text of a certain type based on data in a _.txt_ file. Here are few prepared styles for you to choose from: \n",
        "\n",
        "\n",
        "| Text Type  | Description | Adress / URL |\n",
        "| ------------- | ------------- | ------------- |\n",
        "| Haikus | A form of 3-line poetry, originating from Japan. | [https://raw.githubusercontent.com/PerceptronV/Apollo-Psi/master/Haikus.txt](https://raw.githubusercontent.com/PerceptronV/Apollo-Psi/master/Haikus.txt) |\n",
        "| Shakespeare | _Everyone knows him_ | [https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt](https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt) |\n",
        "| Emily Dickinson | American poet. Her works were unconventional and often centred around themes like death or religion. | [https://raw.githubusercontent.com/PerceptronV/Miscellaneous/master/Emily_Dickinson.txt](https://raw.githubusercontent.com/PerceptronV/Miscellaneous/master/Emily_Dickinson.txt) |\n",
        "| Virgil | __Latin__ full text from the Roman epic poem _The Aeneid_ by Virgil | [https://raw.githubusercontent.com/PerceptronV/Miscellaneous/master/aeneid-lat.txt](https://raw.githubusercontent.com/PerceptronV/Miscellaneous/master/aeneid-lat.txt) |\n",
        "\n",
        "\n",
        "\n",
        "Choose the type you'd like, copy its link address, then run the next block and paste in the url.\n",
        "\n",
        "_(To customise, you can also input an url of any text file you like, as long as it is encoded in utf-8 and is public.)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4QvG7iiAODz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "URL = input(\"Enter the URL of the text file you'd like to use:\\n\")\n",
        "\n",
        "if os.path.exists('src.txt'):\n",
        "  os.remove('src.txt')\n",
        "fp = keras.utils.get_file('src.txt', URL, cache_subdir = '/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2_6p2ZvbSu8",
        "colab_type": "text"
      },
      "source": [
        "###Read and tokenize the text\n",
        "Before we do anything, we have to load the text in the file you've downloaded. Then we convert all the characters in that text into integers, called _tokens_. The whole process is called _tokenization_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cyNk0myXjij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = open(fp,'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "TOKEN_SIZE = len(vocab)\n",
        "\n",
        "get_tok = {u:i for i, u in enumerate(vocab)}\n",
        "get_char = np.array(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz19neRUaD-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's visualize the mapping between the first set of tokens and characters\n",
        "# \\n is the newline character\n",
        "\n",
        "print('token\\t----->\\tcorresponding character\\n')\n",
        "for i in range(0,7):\n",
        "  print('{}\\t----->\\t{}'.format(i, get_char[i].replace('\\n', '\\\\n')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6NTltYhdxtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining helper functions to make tokenizing and decoding easier\n",
        "\n",
        "def tokenize(s):\n",
        "  return [get_tok[i] for i in s]\n",
        "\n",
        "def decode(l):\n",
        "  ret=''\n",
        "  for i in l:\n",
        "    ret+=get_char[i]\n",
        "  return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYpzfxojcUJt",
        "colab_type": "text"
      },
      "source": [
        "###Creating a dataset\n",
        "The training process is really simple. We separate the text into sequences of tokens, all of the same length. We then give the model an input token, and train it to predict the most probable next token.\n",
        "\n",
        "This is like the next-word prediction function on your smartphones, just specialized in the text you're giving it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80lCWotIbP4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the whole text into tokens and turning it into tf.data.Dataset\n",
        "\n",
        "token_text = tokenize(text)\n",
        "token_data = tf.data.Dataset.from_tensor_slices(token_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ft_WHyKdKbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Breaking the text into sequences\n",
        "\n",
        "SEQ_LEN = 100   # This is the sequence length. You can tweak this value.\n",
        "\n",
        "seq_data = token_data.batch(SEQ_LEN, drop_remainder = True)\n",
        "\n",
        "print('Example sequence:\\n')\n",
        "for i in seq_data.take(1):\n",
        "  print(decode(i).replace('\\n', '\\\\n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d8ktDn9eOhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SZ = 64  # The batch-size parameter. It's usually between 64-256.\n",
        "\n",
        "# Creating input-target pairs\n",
        "\n",
        "def to_inp_targ(seq):\n",
        "  '''\n",
        "    Function for turning an individual sequence into input-target pairs\n",
        "  '''\n",
        "  return seq[:-1], seq[1:]\n",
        "\n",
        "# Applying the to_inp_targ function to all sequences, then batching the dataset\n",
        "dataset = seq_data.map(to_inp_targ)\n",
        "batch_dataset = dataset.batch(BATCH_SZ, drop_remainder = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNjBAlzxxDxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualizing input-target pairs\n",
        "\n",
        "for inp_seq, out_seq in dataset.take(1):\n",
        "  print('Input')\n",
        "  print(decode(inp_seq).replace('\\n', '\\\\n'))\n",
        "  print('\\nOutput')\n",
        "  print(decode(out_seq).replace('\\n', '\\\\n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsdoi4Fdft-F",
        "colab_type": "text"
      },
      "source": [
        "###Making the model\n",
        "We're going make our own Model class with keras.Model.\n",
        "\n",
        "The model takens in a token, turns it into an embedding vector, then passes it through a Recurrent Neural Network, a GRU in this case. It then passes through a fully connected neural network, the size of which equals the number of tokens (i.e. the number of different characters we have in our text).\n",
        "\n",
        "This is because we want the model to output a probability distribution on all the possible next tokens, hence achieving next-word prediction.\n",
        "\n",
        "Here is a visual diagram of how the model will operate:\n",
        "![Diagram of next-word-prediction framework.](https://www.tensorflow.org/tutorials/text/images/text_generation_sampling.png)\n",
        "\n",
        "_Image Copyright 2019 The TensorFlow Authors._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvhZ6GL8jETZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model parameters\n",
        "\n",
        "EMB_DIM = 1024  # This is the size of the embedding vector\n",
        "UNITS = 1024  # This is the number of RNN units in the model\n",
        "\n",
        "'''\n",
        "0 <= dropout < 1\n",
        "\n",
        "Dropout controlles how much the model forgets certain information \n",
        "during training.\n",
        "\n",
        "I.e.  A dropout of 0 means that the model remembers everything its \n",
        "      taught at evert moment in the training.\n",
        "\n",
        "Dropout is used to combat overfitting, which happens when your model\n",
        "learns the data so well it begins to memorize it. \n",
        "\n",
        "So, if you find that after training below, the model memorises the\n",
        "dataset its given instead of generating anything new, try increasing\n",
        "the dropout value.\n",
        "'''\n",
        "DROPOUT = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T_H8ZKtesai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(keras.Model):\n",
        "  def __init__(self, embedding_dim, units, token_size, dropout):\n",
        "    super(Model,self).__init__(name = 'Baseline')\n",
        "    \n",
        "    # Defining the layers we need\n",
        "    self.embedding = keras.layers.Embedding(token_size, embedding_dim)\n",
        "    self.gru1 = keras.layers.GRU(units, recurrent_initializer = 'glorot_uniform',\n",
        "                                 return_sequences = True, dropout = dropout)\n",
        "    self.out = keras.layers.Dense(token_size, activation = keras.activations.softmax)\n",
        "\n",
        "  def call(self, input):\n",
        "    # Passing the input forward through our layers\n",
        "\n",
        "    x = self.embedding(input)\n",
        "    x = self.gru1(x)\n",
        "    return self.out(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SYUFKKOklo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(EMB_DIM, UNITS, TOKEN_SIZE, DROPOUT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_UzN5Fpk6ON",
        "colab_type": "text"
      },
      "source": [
        "###Defining the loss function and optimizer\n",
        "Since the task is inherently about classifying the most probable next token, we use the cross-entropy loss.\n",
        "\n",
        "Cross-entropy loss formula:\n",
        "\n",
        "$-\\sum\\limits_{i=1}^M y_i\\log(p_i)$\n",
        "\n",
        "$y_i$ is the label of the $i^{th}$ token. In classification, this value is either 0 or 1, where 1 means that the token is the correct prediction, and 0 states otherwise.\n",
        "\n",
        "$p_i$ is the probability assigned by the model on the $i^{th}$ token.\n",
        "\n",
        "$M$ is the token size.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "As for the optimizer, we will use the Adam Optimizing Algorithm to help train our model weights. More information of Adam is [available here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20an%20optimization%20algorithm,iterative%20based%20in%20training%20data.&text=The%20algorithm%20is%20called%20Adam.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7A1l9Zvm6WJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_function = keras.losses.sparse_categorical_crossentropy\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Remark: There's nothing magical about the name \n",
        "# `sparse_categorical_crossentropy`. It's just some technical\n",
        "# details. The loss function is still calculating the \n",
        "# cross-entropy loss by the formula given above."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aGlEqkprdeJ",
        "colab_type": "text"
      },
      "source": [
        "###Training our network\n",
        "We are going to implement the training loop and gradient descent here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fdZQMKjsgrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(input, labels):\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(input)\n",
        "    loss = loss_function(labels, predictions)\n",
        "  \n",
        "  # calculate gradients with respect to model variables\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # update gradients with optimzer, based on the gradients\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  return tf.reduce_mean(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCN600Uz8Han",
        "colab_type": "text"
      },
      "source": [
        "We're just iterating over the data in the code below. Nothing mysterious."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH29hizZtsg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs, steps_per_epoch):\n",
        "\n",
        "  for i in range(epochs):\n",
        "    total_loss = []\n",
        "    start = time.time()\n",
        "\n",
        "    for step, batch in enumerate(batch_dataset.repeat()):\n",
        "      # Due to the way the dataset is structured, each batch includes\n",
        "      # two components: batch[0] is the inputs, batch[1] is the labels\n",
        "\n",
        "      step_loss = train_step(batch[0], batch[1])\n",
        "      total_loss.append(step_loss)\n",
        "\n",
        "      if step % 30 == 0:\n",
        "        print('Step: {}\\tElapsed time: {}s\\tLoss: {}'.format(\n",
        "            step, time.time()-start, np.mean(np.asarray(total_loss))\n",
        "        ))\n",
        "      \n",
        "      if (step+1) == steps_per_epoch:\n",
        "        # Break out of training loop once the set number of gradient\n",
        "        # descent steps is reached\n",
        "        break\n",
        "    \n",
        "    print('> Epoch: {}\\tTime: {}s\\tLoss: {}\\n'.format(\n",
        "        i+1, time.time()-start, np.mean(np.asarray(total_loss))\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BykoGmFO97yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Epochs is the number of rounds we train an AI. Generally, the more you\n",
        "train, the better. But there is a point at which the model starts to\n",
        "overfit, or in other words, starts to model the data so well it memorises\n",
        "it. \n",
        "\n",
        "If overfitting happens to you, try reducing the number of epochs or tweak\n",
        "the dropout parameter above.\n",
        "\n",
        "If the AI isn't generating meaningul stuff after you've trained it, try\n",
        "increasing the number of epochs, or change the model structure altogether.\n",
        "'''\n",
        "\n",
        "EPOCHS = 30\n",
        "STEPS = 128  # The number of gradient descent steps to perform in each epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y327tlJ_9x7f",
        "colab_type": "text"
      },
      "source": [
        "###_Actually_ running the __`train`__ function\n",
        "Behold - the FUN!!!\n",
        "\n",
        "But, training does take a while. Be patient. Watch the loss drop - it's most satisfying."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB3Qn1si-iv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(EPOCHS, STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYgooUvvC0LJ",
        "colab_type": "text"
      },
      "source": [
        "##Generating text!!\n",
        "Let's get started with our system!\n",
        "\n",
        "---\n",
        "\n",
        "The following code inputs a prompt to the model, and then samples a token from the probability distribution generated by our model. \n",
        "\n",
        "The distribution is divided by the temperature. A higher temperature means there is a higher chance that some tokens with a lower assigned popability might be sampled, often making the generated text surprising. A temperature of 0 is basically the same as selecting the token with highest probability.\n",
        "\n",
        "Temperature is always a non-negative integer. Tweak around to generate balance the creativity with the uniformity of the text.\n",
        "\n",
        "Once a token is sampled, it is added to the original prompt, and this continued prompt is then fed back into the model, which then generates a probability distribution for us to sample from, and so we have another token, and so on etc.\n",
        "\n",
        "This generation technique continues until the model has generated the desired number of outputs, the `generation_length`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrUJKpLkC6rv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(temperature, generation_length, prompt):\n",
        "    \n",
        "    # Turning input text into tokens\n",
        "    input = tf.expand_dims(tokenize(prompt), 0)\n",
        "\n",
        "    output = []  # Initializing a list to store generated tokens\n",
        "    model.reset_states()  # Resetting the idden states of our RNN model\n",
        "\n",
        "    for i in range(generation_length):\n",
        "      \n",
        "      preds = model(input)  # Making a next-word prediction with our model\n",
        "      preds = tf.squeeze(preds, 0)\n",
        "      preds = preds / temperature  # Dividing distribution by temperature\n",
        "\n",
        "      # Sampling from the distribution\n",
        "      id = tf.random.categorical(preds, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      input = tf.expand_dims([id],0)\n",
        "      output.append(id)\n",
        "      \n",
        "    return decode(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDE3Wl0ED_iq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining generation parameters\n",
        "\n",
        "TEMPERATURE = 0.4\n",
        "GENERATION_LENGTH = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC3oDtE6EHrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prompt = '\\n\\nfireflies\\n'\n",
        "generate_text(TEMPERATURE, GENERATION_LENGTH, prompt)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}